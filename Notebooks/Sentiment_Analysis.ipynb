{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "W_MjTA6MvEYa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7486eb5a-8b7b-42d8-a9ca-8f705fd87043"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "import configparser\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import os.path\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import logging\n",
        "import json\n",
        "from datetime import datetime\n"
      ],
      "metadata": {
        "id": "wMEDjZh0uBs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***SENTIMENT ANALYSIS***"
      ],
      "metadata": {
        "id": "pclE2Nz1vnV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "uvg2zFh7vfjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_sentiment(tweet):\n",
        "    encoded_input = tokenizer(tweet, return_tensors='pt')\n",
        "    output = model(**encoded_input)\n",
        "    scores = output.logits.detach().numpy()[0]\n",
        "    return scores\n",
        "   "
      ],
      "metadata": {
        "id": "wbPbLnpzu3d9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for file_name in file_list:\n",
        "    # read the file into a dataframe\n",
        "    df_full_file = pd.read_csv(file_name)\n",
        "\n",
        "    # apply the code to the dataframe\n",
        "    df_new = pd.DataFrame()\n",
        "    df_old = pd.DataFrame()\n",
        "    if 'Sentiment' in df_full_file.columns:\n",
        "        mask = df_full_file['Sentiment'].isnull()\n",
        "        df_new = df_full_file[mask]\n",
        "        df_old = df_full_file[~mask]\n",
        "\n",
        "        df = pd.DataFrame(columns=df_full_file.columns)\n",
        "        for index, row in df_new.iterrows():\n",
        "            tweet_id = row['Id']\n",
        "            if tweet_id in df_old['Id'].values:\n",
        "                # Update retweet and like count\n",
        "                df_old.loc[df_old['Id'] == tweet_id, 'Retweet Count'] = row['Retweet Count'] \n",
        "                df_old.loc[df_old['Id'] == tweet_id, 'Favorite Count'] = row['Favorite Count']\n",
        "                df_old.loc[df_old['Id'] == tweet_id, 'Total Tweets Count'] = int(row['Retweet Count'] + 1)\n",
        "            else:\n",
        "                df = pd.concat([df, row.to_frame().T])    \n",
        "    else:\n",
        "        df = df_full_file\n",
        "\n",
        "    # write the modified dataframe back to the file\n",
        "    df.to_csv(file_name, index=False)\n"
      ],
      "metadata": {
        "id": "tZT5VujK17IE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for file in file_list:\n",
        "    # read the file into a dataframe\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # apply sentiment analysis to the tweets in the dataframe\n",
        "    start_time = time.time()\n",
        "    df[\"Sentiment Score\"] = df[\"Tweet\"].apply(lambda tweet: find_sentiment(tweet))\n",
        "    df[\"Sentiment\"] = df[\"Sentiment Score\"].apply(lambda score: int(np.argmax(score)))\n",
        "    end_time = time.time()\n",
        "    print(\"Time required for sentiment analysis: \", end_time-start_time)\n",
        "\n",
        "    # replace numerical sentiment labels with corresponding string values\n",
        "    dict1 = {0:\"Negative\",1:\"Neutral\",2:\"Positive\"}\n",
        "    df.replace({\"Sentiment\":dict1},inplace=True)\n",
        "\n",
        "    # create columns for positive, neutral, and negative sentiment scores\n",
        "    df[\"Positive Sentiment Score\"] = df[\"Sentiment Score\"].apply(lambda sentiment: sentiment[2])\n",
        "    df[\"Neutral Sentiment Score\"] = df[\"Sentiment Score\"].apply(lambda sentiment: sentiment[1])\n",
        "    df[\"Negative Sentiment Score\"] = df[\"Sentiment Score\"].apply(lambda sentiment: sentiment[0])\n",
        "\n",
        "    # write the modified dataframe back to the file\n",
        "    df.to_csv(file, index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gxa6Z5_f3Xat",
        "outputId": "355dc21e-3eae-4cb6-dc20-1f476b1512f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time required for sentiment analysis:  674.9911568164825\n"
          ]
        }
      ]
    }
  ]
}